{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This cell below is the starter code provided by Udacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"./tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi',\n",
    "                 'total_stock_value',\n",
    "                 'total_payments',\n",
    "                 'restricted_stock',\n",
    "                 'exercised_stock_options',\n",
    "                 'salary',\n",
    "                 'expenses',\n",
    "                 'other',\n",
    "                 'to_messages',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'from_messages',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'bonus']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a variety of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:\n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "# I don't need to dump it right now.\n",
    "# dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Investigate amount of usable data for each data point\n",
    "TOTAL_ITEMS = len(data_dict[data_dict.keys()[0]].keys()) - 1 # minus 1 to exclude POI indicator\n",
    "proportion_unusable_data = {}\n",
    "for person, data in data_dict.items():\n",
    "    num_NaN = 0\n",
    "    for key, value in data.items():\n",
    "        if key != 'poi':\n",
    "            if value == 'NaN':\n",
    "                num_NaN += 1\n",
    "    proportion_unusable_data[person] = num_NaN / float(TOTAL_ITEMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(proportion_unusable_data.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's take a look at our data set and get familiar with our data points\n",
    "num_data_points = len(data_dict)\n",
    "\n",
    "num_poi, num_no_poi = 0, 0\n",
    "for person in data_dict.keys():\n",
    "    if data_dict[person]['poi'] == 0:\n",
    "        num_no_poi += 1\n",
    "    else:\n",
    "        num_poi += 1\n",
    "\n",
    "print(\"Number of data points: \" + str(num_data_points))\n",
    "print(\"Number of POI: \" + str(num_poi))\n",
    "print(\"Number of non-POI: \" + str(num_no_poi))\n",
    "print(\"Proportion of POI to non-POI: \" + str(num_poi/float(num_no_poi)))\n",
    "\n",
    "# Count total features available \"out of the box\"\n",
    "features_list = data_dict[data_dict.keys()[0]].keys()\n",
    "print(\"Number of features available at the start: \"\n",
    "      + str(len(features_list) - 1))    # minus one to exclude 'poi' label\n",
    "                                        # which is not a feature\n",
    "\n",
    "# Are there features with many missing values?\n",
    "missing_values = []\n",
    "\n",
    "for feature in [f for f in features_list if f != 'poi']:\n",
    "    num_nan = 0\n",
    "    for person in data_dict.keys():\n",
    "        feature_value = data_dict[person][feature]\n",
    "        if feature_value == 'NaN':  # Missing values are encoded as 'NaN'\n",
    "            num_nan += 1\n",
    "    print(\"Feature '\"+feature+\"' has \"+str(num_nan)+\" missing values out of \"\n",
    "          +str(num_data_points)+\", or \"+str(num_nan/float(num_data_points))+\"\")\n",
    "    missing_values.append((feature, num_nan, num_data_points, num_nan/float(num_data_points)))\n",
    "\n",
    "\n",
    "# Prep report table\n",
    "print(\"| Feature | # Missing Values | Proportion of Missing Values |\") # header\n",
    "print(\"|:--|:--|:--|\") # formatting\n",
    "# Sort list of features according to number of missing values:\n",
    "for row in sorted(missing_values, key=lambda x: x[3]):\n",
    "    print(\"| \"+str(row[0])+\" | \"+str(row[1])+\" | \"+str(round(row[3], 3))+\" |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(features_train, labels_train)\n",
    "predictions = clf.predict(features_test)\n",
    "clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test for errors:\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print('Precision: '+str(precision_score(labels_test, predictions)))\n",
    "print('Recall: '+str(recall_score(labels_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_features(features_list):\n",
    "    \"Auxiliary function to load new features from data set.\"\n",
    "    ### The first feature in features_list must be \"poi\".\n",
    "    ### Extract features and labels from dataset for local testing\n",
    "    data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "\n",
    "    return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_features(features, labels, features_list):\n",
    "    ### The first feature in features_list must be \"poi\".\n",
    "\n",
    "    # Color POIs as red and non-POIs as blue\n",
    "    color_map = map(lambda x: 'b' if x == 0 else 'r', labels)\n",
    "\n",
    "    plt.scatter([f[0] for f in features],\n",
    "               [f[1] for f in features],\n",
    "               c=color_map)\n",
    "    plt.xlabel(features_list[1])\n",
    "    plt.ylabel(features_list[2])\n",
    "\n",
    "\n",
    "def get_n_max_val(features, features_list, n=1):\n",
    "    \"\"\"Return n maximum values for features.\n",
    "    The first feature in features_list must be \"poi\".\"\"\"\n",
    "    \n",
    "    max_val = []\n",
    "    for i in range(len(features_list[1:])):\n",
    "        values = [feat[i] for feat in features]\n",
    "        max_val.append((features_list[i+1], sorted(values)[-n:]))\n",
    "        \n",
    "    return max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_of_features_lists = [\n",
    "    ['poi','total_stock_value', 'total_payments'],\n",
    "    ['poi', 'restricted_stock', 'exercised_stock_options'],\n",
    "    ['poi', 'salary', 'expenses']\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(6,7))\n",
    "\n",
    "i = 1  # subplot index\n",
    "for features_list in list_of_features_lists:\n",
    "    labels, features = load_features(features_list)\n",
    "    \n",
    "    plt.subplot(3,1,i)\n",
    "    plot_features(features, labels, features_list)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "    # Print max value for current features\n",
    "    print(get_n_max_val(features, features_list))\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('exploration_outliers.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for person, values in data_dict.items():\n",
    "    if values['salary'] == 26704229.0:\n",
    "        print person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove TOTAL from our data set\n",
    "data_dict.pop('TOTAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out RFE; didn't quite work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try some RFE\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "selector = RFECV(clf)\n",
    "selector = selector.fit(features_train, labels_train)\n",
    "\n",
    "print(\"Num Features: %d\") % selector.n_features_\n",
    "print(\"Selected Features: %s\") % selector.support_\n",
    "print(\"Feature Ranking: %s\") % selector.ranking_\n",
    "\n",
    "selector.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out SelectPercentile + SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, SelectKBest, f_classif\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selector = SelectPercentile(f_classif, percentile=20)\n",
    "selector = SelectKBest(f_classif, k=3)\n",
    "selector.fit(features_train, labels_train)\n",
    "\n",
    "sorted(zip(selector.scores_, features_list[1:]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using GaussianNB with ALL features\n",
    "clf = GaussianNB()\n",
    "\n",
    "clf.fit(features_train, labels_train)\n",
    "predictions = clf.predict(features_test)\n",
    "clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test for errors:\n",
    "print('Precision: '+str(precision_score(labels_test, predictions)))\n",
    "print('Recall: '+str(recall_score(labels_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using GaussianNB with SELECTED features\n",
    "clf_selected = GaussianNB()\n",
    "\n",
    "transformed_features_train = selector.transform(features_train)\n",
    "transformed_features_test = selector.transform(features_test)\n",
    "\n",
    "clf_selected.fit(transformed_features_train, labels_train)\n",
    "pred_selected = clf_selected.predict(transformed_features_test)\n",
    "clf_selected.score(transformed_features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test for errors:\n",
    "print('Precision: '+str(precision_score(labels_test, pred_selected)))\n",
    "print('Recall: '+str(recall_score(labels_test, pred_selected)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trying Decision Trees + AdaBoost\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_estimators = 50\n",
    "clf_selected = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=6, min_samples_leaf=1),\n",
    "    n_estimators=n_estimators)\n",
    "\n",
    "transformed_features_train = selector.transform(features_train)\n",
    "transformed_features_test = selector.transform(features_test)\n",
    "\n",
    "clf_selected.fit(transformed_features_train, labels_train)\n",
    "pred_selected = clf_selected.predict(transformed_features_test)\n",
    "clf_selected.score(transformed_features_test, labels_test)\n",
    "\n",
    "# clf_selected.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test for errors:\n",
    "print('Precision: '+str(precision_score(labels_test, pred_selected)))\n",
    "print('Recall: '+str(recall_score(labels_test, pred_selected)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_estimators = 50\n",
    "clf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=6, min_samples_leaf=1),\n",
    "    n_estimators=n_estimators)\n",
    "\n",
    "clf.fit(features_train, labels_train)\n",
    "predictions = clf.predict(features_test)\n",
    "clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test for errors:\n",
    "print('Precision: '+str(precision_score(labels_test, predictions)))\n",
    "print('Recall: '+str(recall_score(labels_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let me try SVMs\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(features_train)\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(scaler.transform(features_train), labels_train)\n",
    "\n",
    "predictions = clf.predict(scaler.transform(features_test))\n",
    "\n",
    "clf.score(scaler.transform(features_test), labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test for errors:\n",
    "print('Precision: '+str(precision_score(labels_test, predictions)))\n",
    "print('Recall: '+str(recall_score(labels_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_selected = SVC()\n",
    "\n",
    "scaled_features_train = scaler.transform(features_train)\n",
    "scaled_features_test = scaler.transform(features_test)\n",
    "\n",
    "clf_selected.fit(selector.transform(scaled_features_train), labels_train)\n",
    "predictions = clf_selected.predict(selector.transform(scaled_features_test))\n",
    "clf_selected.score(selector.transform(scaled_features_test), labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test for errors:\n",
    "print('Precision: '+str(precision_score(labels_test, predictions)))\n",
    "print('Recall: '+str(recall_score(labels_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clf_selected = SVC(C=0.1)\n",
    "# clf_selected = SVC(C=1)\n",
    "# clf_selected = SVC(C=10)\n",
    "# clf_selected = SVC(C=100)\n",
    "\n",
    "clf_selected = SVC(gamma=0.01)\n",
    "# clf_selected = SVC(gamma=1)\n",
    "# clf_selected = SVC(gamma=10)\n",
    "# clf_selected = SVC(gamma=100)\n",
    "\n",
    "scaled_features_train = scaler.transform(features_train)\n",
    "scaled_features_test = scaler.transform(features_test)\n",
    "\n",
    "clf_selected.fit(selector.transform(scaled_features_train), labels_train)\n",
    "predictions = clf_selected.predict(selector.transform(scaled_features_test))\n",
    "clf_selected.score(selector.transform(scaled_features_test), labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'kernel':('linear', 'rbf'),\n",
    "    'C':[1, 10, 100]\n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "clf_selected = GridSearchCV(svc, parameters, scoring='f1')\n",
    "\n",
    "scaled_features_train = scaler.transform(features_train)\n",
    "scaled_features_test = scaler.transform(features_test)\n",
    "\n",
    "clf_selected.fit(selector.transform(scaled_features_train), labels_train)\n",
    "predictions = clf_selected.predict(selector.transform(scaled_features_test))\n",
    "clf_selected.score(selector.transform(scaled_features_test), labels_test)\n",
    "\n",
    "clf_selected.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Trying out StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iter_num = 0\n",
    "\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "\n",
    "for train_index, test_index in sss:\n",
    "    features_train = []\n",
    "    features_test  = []\n",
    "    labels_train   = []\n",
    "    labels_test    = []\n",
    "    \n",
    "    for ii in train_index:\n",
    "        features_train.append( features[ii] )\n",
    "        labels_train.append( labels[ii] )\n",
    "    for jj in test_index:\n",
    "        features_test.append( features[jj] )\n",
    "        labels_test.append( labels[jj] )\n",
    "        \n",
    "    # Select features\n",
    "    selector = SelectKBest(f_classif, k=3)\n",
    "    selector.fit(features_train, labels_train)\n",
    "    print('Features: '+str(sorted(zip(selector.scores_, features_list[1:]), reverse=True)[:3]))\n",
    "    \n",
    "    # Classifier\n",
    "#     clf_selected = DecisionTreeClassifier(random_state=42)\n",
    "    clf_selected = GaussianNB()\n",
    "\n",
    "    selected_features_train = selector.transform(features_train)\n",
    "    selected_features_test = selector.transform(features_test)\n",
    "\n",
    "    clf_selected.fit(selected_features_train, labels_train)\n",
    "    pred_selected = clf_selected.predict(selected_features_test)\n",
    "    \n",
    "    # Test for errors:\n",
    "    precision = precision_score(labels_test, pred_selected)\n",
    "    recall = recall_score(labels_test, pred_selected)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    \n",
    "    print('Iteration: '+str(iter_num))\n",
    "    print('Precision: '+str(precision))\n",
    "    print('Recall: '+str(recall))\n",
    "    print\n",
    "    \n",
    "    iter_num += 1\n",
    "\n",
    "print('Average precison: '+str(np.mean(precision_list)))\n",
    "print('Average recall: '+str(np.mean(recall_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"./tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initializing dataset for local testing\n",
    "features_list = ['poi',\n",
    "                 'total_stock_value',\n",
    "                 'total_payments',\n",
    "                 'restricted_stock',\n",
    "                 'exercised_stock_options',\n",
    "                 'salary',\n",
    "                 'expenses',\n",
    "                 'other',\n",
    "                 'to_messages',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'from_messages',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'bonus']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "data_dict.pop('TOTAL')\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "def test_classifier(clf, data_sets):\n",
    "    iter_num = 0\n",
    "\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "\n",
    "    for data_set in data_sets:\n",
    "        features_train, labels_train, features_test, labels_test = data_set\n",
    "\n",
    "        # Select features\n",
    "        selector = SelectKBest(f_classif, k=3)\n",
    "        selector.fit(features_train, labels_train)\n",
    "        print('Features: '+str(sorted(zip(selector.scores_, features_list[1:]), reverse=True)[:3]))\n",
    "\n",
    "        selected_features_train = selector.transform(features_train)\n",
    "        selected_features_test = selector.transform(features_test)\n",
    "\n",
    "        # Fit classifier and make predictions\n",
    "        clf.fit(selected_features_train, labels_train)\n",
    "        pred_selected = clf.predict(selected_features_test)\n",
    "\n",
    "        # Test for errors:\n",
    "        precision = precision_score(labels_test, pred_selected)\n",
    "        recall = recall_score(labels_test, pred_selected)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "        print('Iteration: '+str(iter_num))\n",
    "        print('Precision: '+str(precision))\n",
    "        print('Recall: '+str(recall))\n",
    "        print\n",
    "\n",
    "        iter_num += 1\n",
    "\n",
    "    print('classifier: '+str(clf))\n",
    "    print('Average precison: '+str(np.mean(precision_list)))\n",
    "    print('Average recall: '+str(np.mean(recall_list)))\n",
    "    print\n",
    "\n",
    "\n",
    "def split_data_sss(features, labels):\n",
    "    '''Uses StratifiedShuffleSplit to create train and test data.\n",
    "    Returns a list of tuples.'''\n",
    "    sss = StratifiedShuffleSplit(y=labels, test_size=0.3, n_iter=10)\n",
    "\n",
    "    train_test_sets = []\n",
    "\n",
    "    for train_index, test_index in sss:\n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "\n",
    "        for i in train_index:\n",
    "            features_train.append(features[i])\n",
    "            labels_train.append(labels[i])\n",
    "        for j in test_index:\n",
    "            features_test.append(features[j])\n",
    "            labels_test.append(labels[j])\n",
    "\n",
    "        train_test_sets.append((features_train, labels_train, features_test, labels_test))\n",
    "\n",
    "    return train_test_sets\n",
    "\n",
    "\n",
    "# Test our classifiers\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(random_state=42)\n",
    "]\n",
    "\n",
    "# Split data into train/test data sets\n",
    "data_sets = split_data_sss(features, labels)\n",
    "\n",
    "for clf in classifiers:\n",
    "    test_classifier(clf, data_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tup = (23.18661338478892, 'exercised_stock_options')\n",
    "tup[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## New feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_proportion_from_poi(data_point):\n",
    "    '''Creates a new feature: The proportion of all emails received by this person\n",
    "    that were sent from a POI.'''\n",
    "\n",
    "    FEATURE_NAME = 'proportion_from_poi'\n",
    "\n",
    "    data_point[FEATURE_NAME] = float(data_point['from_poi_to_this_person'])/float(data_point['to_messages'])\n",
    "\n",
    "\n",
    "def update_data_dict_proportion_from_poi(data_dict):\n",
    "    '''Updates data_dict with new feature.'''\n",
    "\n",
    "    for data in data_dict.values():\n",
    "        create_proportion_from_poi(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_data_dict_proportion_from_poi(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "proportion_from_poi_list = []\n",
    "for person, data in data_dict.items():\n",
    "    proportion_from_poi_list.append((person, data['proportion_from_poi']))\n",
    "    \n",
    "sorted(proportion_from_poi_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
